#!/usr/bin/env python3
"""
Flash Attention Plus å¼€å‘ç¯å¢ƒéªŒè¯è„šæœ¬
è¿è¡Œæ­¤è„šæœ¬ç¡®ä¿å¼€å‘ç¯å¢ƒè®¾ç½®æ­£ç¡®
"""

import sys
import os
import torch
import traceback

def check_environment():
    """æ£€æŸ¥å¼€å‘ç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥å¼€å‘ç¯å¢ƒ...")
    
    # æ£€æŸ¥ Python ç‰ˆæœ¬
    python_version = sys.version_info
    print(f"Python ç‰ˆæœ¬: {python_version.major}.{python_version.minor}.{python_version.micro}")
    if python_version.major != 3 or python_version.minor < 9:
        print("âš ï¸  è­¦å‘Š: æ¨èä½¿ç”¨ Python 3.9+")
    else:
        print("âœ… Python ç‰ˆæœ¬åˆé€‚")
    
    # æ£€æŸ¥ CUDA
    if torch.cuda.is_available():
        print(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name()}")
        print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   å¯ç”¨ GPU æ•°é‡: {torch.cuda.device_count()}")
    else:
        print("âŒ CUDA ä¸å¯ç”¨")
        return False
    
    return True

def check_flaggems():
    """æ£€æŸ¥ FlagGems æ˜¯å¦å¯ç”¨"""
    print("\nğŸ” æ£€æŸ¥ FlagGems...")
    
    try:
        # æ·»åŠ  FlagGems è·¯å¾„
        flaggems_path = "/home/Master/YangKY/.code/library/FlagGems/src"
        if flaggems_path not in sys.path:
            sys.path.insert(0, flaggems_path)
        
        import flag_gems
        print("âœ… FlagGems å¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥å…·ä½“çš„æ³¨æ„åŠ›å‡½æ•°
        try:
            from flag_gems.ops.attention import flash_attention_forward
            print("âœ… flash_attention_forward å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  flash_attention_forward å¯¼å…¥å¤±è´¥: {e}")
        
        try:
            from flag_gems.ops.flash_api import mha_fwd
            print("âœ… mha_fwd å¯ç”¨")
        except ImportError as e:
            print(f"âš ï¸  mha_fwd å¯¼å…¥å¤±è´¥: {e}")
            
        return True
        
    except ImportError as e:
        print(f"âŒ FlagGems å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿ FlagGems å·²æ­£ç¡®å®‰è£…å¹¶ä¸”è·¯å¾„è®¾ç½®æ­£ç¡®")
        return False

def check_flash_attention_plus():
    """æ£€æŸ¥ Flash Attention Plus å®ç°"""
    print("\nğŸ” æ£€æŸ¥ Flash Attention Plus...")
    
    try:
        import flash_attn
        print("âœ… flash_attn æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥ä¸»è¦å‡½æ•°æ˜¯å¦å¯ç”¨
        functions_to_check = [
            'flash_attn_func',
            'flash_attn_qkvpacked_func', 
            'flash_attn_kvpacked_func',
            'flash_attn_varlen_func',
            'flash_attn_with_kvcache',
        ]
        
        for func_name in functions_to_check:
            if hasattr(flash_attn, func_name):
                print(f"âœ… {func_name} å¯ç”¨")
            else:
                print(f"âŒ {func_name} ä¸å¯ç”¨")
                
        return True
        
    except ImportError as e:
        print(f"âŒ flash_attn æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_forward_functions():
    """æµ‹è¯•å‰å‘ä¼ æ’­å‡½æ•°"""
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­å‡½æ•°...")
    
    try:
        import flash_attn
        device = torch.device('cuda')
        
        # æµ‹è¯•æ ‡å‡†æ³¨æ„åŠ›
        print("æµ‹è¯• flash_attn_func...")
        q = torch.randn(2, 64, 4, 32, dtype=torch.float16, device=device)
        k = torch.randn(2, 64, 4, 32, dtype=torch.float16, device=device)
        v = torch.randn(2, 64, 4, 32, dtype=torch.float16, device=device)
        
        out = flash_attn.flash_attn_func(q, k, v, causal=True)
        print(f"âœ… flash_attn_func è¾“å‡ºå½¢çŠ¶: {out.shape}")
        
        # æµ‹è¯• QKV æ‰“åŒ…
        print("æµ‹è¯• flash_attn_qkvpacked_func...")
        qkv = torch.randn(2, 64, 3, 4, 32, dtype=torch.float16, device=device)
        out_qkv = flash_attn.flash_attn_qkvpacked_func(qkv, causal=True)
        print(f"âœ… flash_attn_qkvpacked_func è¾“å‡ºå½¢çŠ¶: {out_qkv.shape}")
        
        # æµ‹è¯•å˜é•¿åºåˆ—
        print("æµ‹è¯• flash_attn_varlen_func...")
        total_len = 128
        q_varlen = torch.randn(total_len, 4, 32, dtype=torch.float16, device=device)
        k_varlen = torch.randn(total_len, 4, 32, dtype=torch.float16, device=device)
        v_varlen = torch.randn(total_len, 4, 32, dtype=torch.float16, device=device)
        cu_seqlens = torch.tensor([0, 32, 64, 96, 128], dtype=torch.int32, device=device)
        
        out_varlen = flash_attn.flash_attn_varlen_func(
            q_varlen, k_varlen, v_varlen, cu_seqlens, cu_seqlens, 32, 32, causal=True
        )
        print(f"âœ… flash_attn_varlen_func è¾“å‡ºå½¢çŠ¶: {out_varlen.shape}")
        
        # æµ‹è¯• KV ç¼“å­˜ï¼ˆåº”è¯¥å¤±è´¥ï¼‰
        print("æµ‹è¯• flash_attn_with_kvcacheï¼ˆæœŸæœ›å¤±è´¥ï¼‰...")
        try:
            q_cache = torch.randn(1, 32, 4, 32, dtype=torch.float16, device=device)
            k_cache = torch.randn(1, 32, 4, 32, dtype=torch.float16, device=device)
            v_cache = torch.randn(1, 32, 4, 32, dtype=torch.float16, device=device)
            
            out_cache = flash_attn.flash_attn_with_kvcache(q_cache, k_cache, v_cache)
            print(f"âŒ flash_attn_with_kvcache åº”è¯¥å¤±è´¥ä½†æˆåŠŸäº†: {out_cache.shape}")
        except NotImplementedError:
            print("âœ… flash_attn_with_kvcache æ­£ç¡®æŠ›å‡º NotImplementedError")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_backward_limitations():
    """æµ‹è¯•åå‘ä¼ æ’­é™åˆ¶ï¼ˆåº”è¯¥å¤±è´¥ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•åå‘ä¼ æ’­é™åˆ¶...")
    
    try:
        import flash_attn
        device = torch.device('cuda')
        
        q = torch.randn(1, 32, 4, 32, dtype=torch.float16, device=device, requires_grad=True)
        k = torch.randn(1, 32, 4, 32, dtype=torch.float16, device=device, requires_grad=True)
        v = torch.randn(1, 32, 4, 32, dtype=torch.float16, device=device, requires_grad=True)
        
        out = flash_attn.flash_attn_func(q, k, v)
        loss = out.sum()
        
        try:
            loss.backward()
            print("âŒ åå‘ä¼ æ’­åº”è¯¥å¤±è´¥ä½†æˆåŠŸäº†")
            return False
        except NotImplementedError:
            print("âœ… åå‘ä¼ æ’­æ­£ç¡®æŠ›å‡º NotImplementedError")
            return True
            
    except Exception as e:
        print(f"âŒ åå‘ä¼ æ’­é™åˆ¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def check_project_structure():
    """æ£€æŸ¥é¡¹ç›®æ–‡ä»¶ç»“æ„"""
    print("\nğŸ” æ£€æŸ¥é¡¹ç›®ç»“æ„...")
    
    required_files = [
        'flash_attn/__init__.py',
        'flash_attn/flash_attn_interface.py',
        'flash_attn/flash_attn_flaggems_backend.py',
        'README.md',
        'README_CN.md', 
        'README_EN.md',
        'IMPLEMENTATION_PLAN_CN.md',
        'TASK_CHECKLIST_CN.md',
        'PROJECT_STATUS_CN.md',
        'IMPLEMENTATION_COMPARISON_CN.md',
        'IMPLEMENTATION_ANALYSIS_CN.md',
        'docs/index.md',
        'docs/api.md',
        'docs/usage.md',
    ]
    
    all_exist = True
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"âœ… {file_path}")
        else:
            print(f"âŒ {file_path} ç¼ºå¤±")
            all_exist = False
    
    return all_exist

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Flash Attention Plus å¼€å‘ç¯å¢ƒéªŒè¯")
    print("=" * 50)
    
    all_checks_passed = True
    
    # åŸºç¡€ç¯å¢ƒæ£€æŸ¥
    if not check_environment():
        all_checks_passed = False
    
    # FlagGems æ£€æŸ¥
    if not check_flaggems():
        all_checks_passed = False
    
    # Flash Attention Plus æ£€æŸ¥
    if not check_flash_attention_plus():
        all_checks_passed = False
    
    # é¡¹ç›®ç»“æ„æ£€æŸ¥
    if not check_project_structure():
        all_checks_passed = False
    
    # åŠŸèƒ½æµ‹è¯•
    if not test_forward_functions():
        all_checks_passed = False
        
    if not test_backward_limitations():
        all_checks_passed = False
    
    print("\n" + "=" * 50)
    if all_checks_passed:
        print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼å¼€å‘ç¯å¢ƒè®¾ç½®æ­£ç¡®ã€‚")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("1. é˜…è¯» README_CN.md äº†è§£é¡¹ç›®æ¦‚å†µ")
        print("2. æŸ¥çœ‹ IMPLEMENTATION_PLAN_CN.md äº†è§£è¯¦ç»†è®¡åˆ’")
        print("3. æŒ‰ç…§ TASK_CHECKLIST_CN.md å¼€å§‹å®ç°")
        print("4. ä» 'ä»»åŠ¡ 2.1.1: FlagGems æºç åˆ†æ' å¼€å§‹")
    else:
        print("âŒ éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œè¯·ä¿®å¤ä¸Šè¿°é—®é¢˜åé‡è¯•ã€‚")
    
    return all_checks_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)