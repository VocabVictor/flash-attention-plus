# Flash Attention Plus ä»»åŠ¡æ¸…å•

## å½“å‰é¡¹ç›®çŠ¶æ€ âœ…

### å·²å®Œæˆçš„ä»»åŠ¡
- [x] ç¯å¢ƒè®¾ç½®å’Œ FlagGems é›†æˆ
- [x] ç§»é™¤ USE_FLAGGEMS æ¡ä»¶åˆ¤æ–­
- [x] å®ç° `_flaggems_flash_attn_forward()` æ ‡å‡†å‰å‘ä¼ æ’­
- [x] å®ç° `_flash_attn_qkvpacked_forward()` QKV æ‰“åŒ…æ ¼å¼
- [x] å®ç° `_flash_attn_kvpacked_forward()` KV æ‰“åŒ…æ ¼å¼  
- [x] å®ç° `_flash_attn_varlen_forward()` å˜é•¿åºåˆ—æ”¯æŒ
- [x] ä¿®å¤é€’å½’è°ƒç”¨é—®é¢˜
- [x] åˆ›å»ºå®Œæ•´çš„æµ‹è¯•å¥—ä»¶
- [x] ç¼–å†™ä¸­æ–‡æ–‡æ¡£å’Œå®ç°è®¡åˆ’

## ä¸‹ä¸€é˜¶æ®µä»»åŠ¡ï¼šåå‘ä¼ æ’­å®ç° ğŸš§

### é˜¶æ®µ 2.1ï¼šFlagGems åå‘ä¼ æ’­è°ƒç ”ï¼ˆé¢„ä¼° 1-2 å‘¨ï¼‰

#### ä»»åŠ¡ 2.1.1ï¼šåˆ†æ FlagGems æºç ç»“æ„
**ä¼˜å…ˆçº§ï¼šé«˜ ğŸ”´**
```bash
# æ‰§è¡Œä»¥ä¸‹å‘½ä»¤äº†è§£ FlagGems åå‘ä¼ æ’­èƒ½åŠ›
cd /home/Master/YangKY/.code/library/FlagGems

# 1. æœç´¢åå‘ä¼ æ’­ç›¸å…³ä»£ç 
find . -name "*.py" -exec grep -l "backward\|grad\|autograd" {} \;

# 2. æŸ¥çœ‹æ³¨æ„åŠ›ç›¸å…³çš„åå‘ä¼ æ’­
find . -name "*.py" -exec grep -l "attention.*backward" {} \;

# 3. åˆ†æ Triton å†…æ ¸å®ç°
find . -name "*.py" -exec grep -l "@triton" {} \;

# 4. æŸ¥çœ‹ autograd é›†æˆ
grep -r "torch.autograd.Function" src/
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] åˆ›å»º `docs/flaggems_backward_analysis.md` æŠ¥å‘Š
- [ ] åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„åå‘ä¼ æ’­å‡½æ•°
- [ ] åˆ†æç°æœ‰çš„ autograd é›†æˆæ¨¡å¼
- [ ] è¯„ä¼°å®ç°åå‘ä¼ æ’­çš„æŠ€æœ¯å¯è¡Œæ€§

#### ä»»åŠ¡ 2.1.2ï¼šç ”ç©¶ Flash Attention åå‘ä¼ æ’­ç®—æ³•
**ä¼˜å…ˆçº§ï¼šé«˜ ğŸ”´**

**å­¦ä¹ èµ„æºï¼š**
1. Flash Attention è®ºæ–‡ä¸­çš„åå‘ä¼ æ’­ç®—æ³•
2. åŸå§‹ CUDA å®ç°çš„åå‘ä¼ æ’­ä»£ç 
3. PyTorch åŸç”Ÿæ³¨æ„åŠ›çš„åå‘ä¼ æ’­

**éœ€è¦ç†è§£çš„æ ¸å¿ƒç®—æ³•ï¼š**
```python
# Flash Attention åå‘ä¼ æ’­çš„æ•°å­¦åŸç†
"""
ç»™å®šï¼š
- dOut: è¾“å‡ºæ¢¯åº¦ (B, L, H, D)
- Q, K, V: è¾“å…¥å¼ é‡
- Softmax_LSE: log-sum-exp å€¼
- Attention_weights: æ³¨æ„åŠ›æƒé‡

è®¡ç®—ï¼š
1. dV = Attention_weights.T @ dOut
2. dAttn = dOut @ V.T  
3. dAttn_softmax = softmax_backward(dAttn, Attention_weights)
4. dScores = dAttn_softmax
5. dQ = dScores @ K * scale
6. dK = dScores.T @ Q * scale
"""
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] åˆ›å»º `docs/flash_attention_backward_algorithm.md`
- [ ] å®ç°ç®—æ³•çš„æ•°å­¦æ¨å¯¼
- [ ] ç”¨ PyTorch å†™å‡ºå‚è€ƒå®ç°
- [ ] åˆ†æå†…å­˜å’Œè®¡ç®—å¤æ‚åº¦

#### ä»»åŠ¡ 2.1.3ï¼šè®¾è®¡ FlagGems åå‘ä¼ æ’­æ¶æ„
**ä¼˜å…ˆçº§ï¼šä¸­ ğŸŸ¡**

**æ–‡ä»¶ç»“æ„è®¾è®¡ï¼š**
```
flash_attn/
â”œâ”€â”€ backward/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ flaggems_backward.py      # FlagGems åå‘ä¼ æ’­æ ¸å¿ƒ
â”‚   â”œâ”€â”€ autograd_functions.py     # PyTorch autograd é›†æˆ
â”‚   â””â”€â”€ reference_impl.py         # PyTorch å‚è€ƒå®ç°
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] è®¾è®¡è¯¦ç»†çš„ API æ¥å£
- [ ] åˆ›å»ºæ¨¡å—æ–‡ä»¶ç»“æ„
- [ ] å®šä¹‰å‡½æ•°ç­¾åå’Œå‚æ•°è§„èŒƒ
- [ ] åˆ¶å®šæµ‹è¯•ç­–ç•¥

### é˜¶æ®µ 2.2ï¼šåŸºç¡€åå‘ä¼ æ’­å®ç°ï¼ˆé¢„ä¼° 2-3 å‘¨ï¼‰

#### ä»»åŠ¡ 2.2.1ï¼šåˆ›å»ºåå‘ä¼ æ’­æ¨¡å—ç»“æ„
**ä¼˜å…ˆçº§ï¼šé«˜ ğŸ”´**

```bash
# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p flash_attn/backward
touch flash_attn/backward/__init__.py
touch flash_attn/backward/flaggems_backward.py
touch flash_attn/backward/autograd_functions.py
touch flash_attn/backward/reference_impl.py
```

**æ–‡ä»¶å†…å®¹æ¨¡æ¿ï¼š**

`flash_attn/backward/__init__.py`:
```python
"""Flash Attention åå‘ä¼ æ’­æ¨¡å—"""

from .flaggems_backward import (
    flaggems_flash_attn_backward,
    flaggems_flash_attn_varlen_backward,
)
from .autograd_functions import (
    FlashAttnFunction,
    FlashAttnVarlenFunction,
)

__all__ = [
    'flaggems_flash_attn_backward',
    'flaggems_flash_attn_varlen_backward', 
    'FlashAttnFunction',
    'FlashAttnVarlenFunction',
]
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] åˆ›å»ºå®Œæ•´çš„æ¨¡å—æ–‡ä»¶ç»“æ„
- [ ] å®ç°åŸºç¡€çš„å¯¼å…¥å’Œå¯¼å‡º
- [ ] æ·»åŠ é€‚å½“çš„æ–‡æ¡£å­—ç¬¦ä¸²

#### ä»»åŠ¡ 2.2.2ï¼šå®ç° PyTorch å‚è€ƒç‰ˆæœ¬
**ä¼˜å…ˆçº§ï¼šé«˜ ğŸ”´**

`flash_attn/backward/reference_impl.py`:
```python
import torch
import torch.nn.functional as F
from typing import Tuple, Optional

def pytorch_flash_attention_backward(
    dout: torch.Tensor,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    out: torch.Tensor,
    softmax_lse: torch.Tensor,
    causal: bool = False,
    softmax_scale: Optional[float] = None,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    PyTorch åŸç”Ÿå®ç°çš„ Flash Attention åå‘ä¼ æ’­
    ç”¨äºéªŒè¯ FlagGems å®ç°çš„æ­£ç¡®æ€§
    """
    # TODO: å®ç°å®Œæ•´çš„åå‘ä¼ æ’­ç®—æ³•
    pass

# å•å…ƒæµ‹è¯•
def test_reference_implementation():
    """æµ‹è¯•å‚è€ƒå®ç°çš„æ­£ç¡®æ€§"""
    # TODO: ä¸ PyTorch åŸç”Ÿ scaled_dot_product_attention å¯¹æ¯”
    pass
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] å®Œæ•´çš„ PyTorch å‚è€ƒå®ç°
- [ ] é€šè¿‡ä¸åŸç”Ÿ PyTorch å‡½æ•°çš„å¯¹æ¯”éªŒè¯
- [ ] å¤„ç†å„ç§è¾¹ç•Œæƒ…å†µï¼ˆcausal, different shapesç­‰ï¼‰

#### ä»»åŠ¡ 2.2.3ï¼šå®ç° FlagGems åå‘ä¼ æ’­æ ¸å¿ƒ
**ä¼˜å…ˆçº§ï¼šé«˜ ğŸ”´**

`flash_attn/backward/flaggems_backward.py`:
```python
import torch
from typing import Tuple, Optional
import sys
import os

# æ·»åŠ  FlagGems è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../../FlagGems/src'))

try:
    from flag_gems.ops.attention import flash_attention_backward
    FLAGGEMS_BACKWARD_AVAILABLE = True
except ImportError:
    FLAGGEMS_BACKWARD_AVAILABLE = False
    print("è­¦å‘Š: FlagGems åå‘ä¼ æ’­ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ PyTorch å‚è€ƒå®ç°")

def flaggems_flash_attn_backward(
    dout: torch.Tensor,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    out: torch.Tensor,
    softmax_lse: torch.Tensor,
    dq: Optional[torch.Tensor] = None,
    dk: Optional[torch.Tensor] = None,
    dv: Optional[torch.Tensor] = None,
    causal: bool = False,
    softmax_scale: Optional[float] = None,
    window_size_left: int = -1,
    window_size_right: int = -1,
    alibi_slopes: Optional[torch.Tensor] = None,
    deterministic: bool = False,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    ä½¿ç”¨ FlagGems åç«¯çš„ Flash Attention åå‘ä¼ æ’­
    """
    if not FLAGGEMS_BACKWARD_AVAILABLE:
        # å›é€€åˆ°å‚è€ƒå®ç°
        from .reference_impl import pytorch_flash_attention_backward
        return pytorch_flash_attention_backward(
            dout, q, k, v, out, softmax_lse, causal, softmax_scale
        )
    
    # TODO: ä½¿ç”¨ FlagGems å®ç°åå‘ä¼ æ’­
    # è¿™é‡Œéœ€è¦è°ƒç”¨ FlagGems çš„åå‘ä¼ æ’­å‡½æ•°
    
    # æš‚æ—¶æŠ›å‡ºæœªå®ç°é”™è¯¯
    raise NotImplementedError("FlagGems åå‘ä¼ æ’­å®ç°å¾…å®Œæˆ")
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] å®Œæ•´çš„å‡½æ•°æ¥å£å®ç°
- [ ] é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶
- [ ] å‚æ•°éªŒè¯å’Œç±»å‹æ£€æŸ¥
- [ ] ä¸ FlagGems API çš„æ­£ç¡®é›†æˆ

#### ä»»åŠ¡ 2.2.4ï¼šå®ç° AutoGrad é›†æˆ
**ä¼˜å…ˆçº§ï¼šé«˜ ğŸ”´**

`flash_attn/backward/autograd_functions.py`:
```python
import torch
from typing import Optional, Tuple
from .flaggems_backward import flaggems_flash_attn_backward

class FlashAttnFunction(torch.autograd.Function):
    """Flash Attention AutoGrad å‡½æ•°ï¼Œæ”¯æŒè‡ªåŠ¨å¾®åˆ†"""
    
    @staticmethod
    def forward(
        ctx,
        q: torch.Tensor,
        k: torch.Tensor, 
        v: torch.Tensor,
        causal: bool = False,
        softmax_scale: Optional[float] = None,
        window_size_left: int = -1,
        window_size_right: int = -1,
        alibi_slopes: Optional[torch.Tensor] = None,
        return_softmax_lse: bool = False,
    ):
        # å‰å‘ä¼ æ’­ - ä½¿ç”¨ç°æœ‰çš„ FlagGems å®ç°
        from ..flash_attn_flaggems_backend import _flaggems_flash_attn_forward
        
        out, softmax_lse, S_dmask, rng_state = _flaggems_flash_attn_forward(
            q, k, v,
            dropout_p=0.0,  # AutoGrad ç‰ˆæœ¬ä¸æ”¯æŒ dropout
            softmax_scale=softmax_scale,
            causal=causal,
            window_size_left=window_size_left,
            window_size_right=window_size_right,
            alibi_slopes=alibi_slopes,
            return_softmax=False,
        )
        
        # ä¿å­˜åå‘ä¼ æ’­éœ€è¦çš„å¼ é‡
        ctx.save_for_backward(q, k, v, out, softmax_lse)
        ctx.causal = causal
        ctx.softmax_scale = softmax_scale
        ctx.window_size_left = window_size_left
        ctx.window_size_right = window_size_right
        ctx.alibi_slopes = alibi_slopes
        
        if return_softmax_lse:
            return out, softmax_lse
        return out
    
    @staticmethod
    def backward(ctx, dout, *args):
        q, k, v, out, softmax_lse = ctx.saved_tensors
        
        # è°ƒç”¨ FlagGems åå‘ä¼ æ’­
        dq, dk, dv = flaggems_flash_attn_backward(
            dout, q, k, v, out, softmax_lse,
            causal=ctx.causal,
            softmax_scale=ctx.softmax_scale,
            window_size_left=ctx.window_size_left,
            window_size_right=ctx.window_size_right,
            alibi_slopes=ctx.alibi_slopes,
        )
        
        return dq, dk, dv, None, None, None, None, None, None

# ä¾¿æ·çš„ API å‡½æ•°
def flash_attn_func_with_grad(
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    causal: bool = False,
    softmax_scale: Optional[float] = None,
    **kwargs
) -> torch.Tensor:
    """æ”¯æŒæ¢¯åº¦çš„ Flash Attention å‡½æ•°"""
    return FlashAttnFunction.apply(q, k, v, causal, softmax_scale, **kwargs)
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] å®Œæ•´çš„ AutoGrad å‡½æ•°å®ç°
- [ ] æ­£ç¡®çš„æ¢¯åº¦ä¼ æ’­
- [ ] é€‚å½“çš„ä¸Šä¸‹æ–‡ä¿å­˜å’Œæ¢å¤
- [ ] ä¾¿æ·çš„ API å°è£…

### é˜¶æ®µ 2.3ï¼šæµ‹è¯•å’ŒéªŒè¯ï¼ˆé¢„ä¼° 1 å‘¨ï¼‰

#### ä»»åŠ¡ 2.3.1ï¼šåˆ›å»ºåå‘ä¼ æ’­æµ‹è¯•å¥—ä»¶
**ä¼˜å…ˆçº§ï¼šé«˜ ğŸ”´**

```bash
# åˆ›å»ºæµ‹è¯•ç›®å½•
mkdir -p tests/backward
touch tests/backward/__init__.py
touch tests/backward/test_backward_correctness.py
touch tests/backward/test_autograd_integration.py
touch tests/backward/test_performance.py
```

`tests/backward/test_backward_correctness.py`:
```python
import torch
import pytest
from flash_attn.backward.autograd_functions import flash_attn_func_with_grad

class TestBackwardCorrectness:
    """æµ‹è¯•åå‘ä¼ æ’­çš„æ•°å€¼æ­£ç¡®æ€§"""
    
    @pytest.mark.parametrize("batch_size,seq_len,num_heads,head_dim", [
        (1, 32, 4, 64),
        (2, 128, 8, 64), 
        (4, 512, 12, 80),
    ])
    def test_gradient_shapes(self, batch_size, seq_len, num_heads, head_dim):
        """æµ‹è¯•æ¢¯åº¦å½¢çŠ¶æ­£ç¡®æ€§"""
        device = torch.device('cuda')
        
        q = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                       device=device, requires_grad=True)
        k = torch.randn(batch_size, seq_len, num_heads, head_dim,
                       device=device, requires_grad=True)
        v = torch.randn(batch_size, seq_len, num_heads, head_dim,
                       device=device, requires_grad=True)
        
        out = flash_attn_func_with_grad(q, k, v)
        loss = out.sum()
        loss.backward()
        
        assert q.grad is not None
        assert k.grad is not None  
        assert v.grad is not None
        assert q.grad.shape == q.shape
        assert k.grad.shape == k.shape
        assert v.grad.shape == v.shape
    
    def test_gradient_finite_difference(self):
        """ä½¿ç”¨æœ‰é™å·®åˆ†éªŒè¯æ¢¯åº¦æ­£ç¡®æ€§"""
        device = torch.device('cuda')
        B, L, H, D = 2, 64, 4, 32
        eps = 1e-3
        
        q = torch.randn(B, L, H, D, device=device, dtype=torch.float64)
        k = torch.randn(B, L, H, D, device=device, dtype=torch.float64)
        v = torch.randn(B, L, H, D, device=device, dtype=torch.float64)
        
        def func(q, k, v):
            return flash_attn_func_with_grad(q, k, v).sum()
        
        # è®¡ç®—æ•°å€¼æ¢¯åº¦
        grad_q_numerical = torch.zeros_like(q)
        for i in range(q.numel()):
            q_plus = q.clone().flatten()
            q_minus = q.clone().flatten()
            q_plus[i] += eps
            q_minus[i] -= eps
            
            q_plus = q_plus.reshape(q.shape)
            q_minus = q_minus.reshape(q.shape)
            
            grad_q_numerical.flatten()[i] = (
                func(q_plus, k, v) - func(q_minus, k, v)
            ) / (2 * eps)
        
        # è®¡ç®—è‡ªåŠ¨å¾®åˆ†æ¢¯åº¦
        q_ad = q.clone().requires_grad_(True)
        k_ad = k.clone().requires_grad_(True)  
        v_ad = v.clone().requires_grad_(True)
        
        out = func(q_ad, k_ad, v_ad)
        out.backward()
        
        # æ¯”è¾ƒæ¢¯åº¦
        torch.testing.assert_close(
            q_ad.grad, grad_q_numerical, rtol=1e-3, atol=1e-3
        )
    
    def test_causal_gradients(self):
        """æµ‹è¯•å› æœæ©ç ä¸‹çš„æ¢¯åº¦æ­£ç¡®æ€§"""
        # TODO: å®ç°å› æœæ©ç æ¢¯åº¦æµ‹è¯•
        pass
        
    def test_vs_pytorch_native(self):
        """ä¸ PyTorch åŸç”Ÿå®ç°å¯¹æ¯”"""
        device = torch.device('cuda')
        B, L, H, D = 2, 32, 4, 64
        
        # ç”Ÿæˆç›¸åŒçš„è¾“å…¥
        torch.manual_seed(42)
        q1 = torch.randn(B, L, H, D, device=device, requires_grad=True)
        k1 = torch.randn(B, L, H, D, device=device, requires_grad=True)
        v1 = torch.randn(B, L, H, D, device=device, requires_grad=True)
        
        torch.manual_seed(42)
        q2 = torch.randn(B, L, H, D, device=device, requires_grad=True)
        k2 = torch.randn(B, L, H, D, device=device, requires_grad=True)
        v2 = torch.randn(B, L, H, D, device=device, requires_grad=True)
        
        # Flash Attention å®ç°
        out1 = flash_attn_func_with_grad(q1, k1, v1)
        loss1 = out1.sum()
        loss1.backward()
        
        # PyTorch åŸç”Ÿå®ç°
        scale = (D ** -0.5)
        scores = torch.matmul(q2, k2.transpose(-2, -1)) * scale
        attn = torch.softmax(scores, dim=-1)
        out2 = torch.matmul(attn, v2)
        loss2 = out2.sum()
        loss2.backward()
        
        # æ¯”è¾ƒç»“æœï¼ˆå…è®¸ä¸€å®šè¯¯å·®ï¼‰
        torch.testing.assert_close(out1, out2, rtol=1e-2, atol=1e-3)
        torch.testing.assert_close(q1.grad, q2.grad, rtol=1e-2, atol=1e-3)
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] å®Œæ•´çš„æµ‹è¯•å¥—ä»¶
- [ ] å½¢çŠ¶æ­£ç¡®æ€§éªŒè¯
- [ ] æ•°å€¼æ­£ç¡®æ€§éªŒè¯ï¼ˆæœ‰é™å·®åˆ†ï¼‰
- [ ] ä¸ PyTorch åŸç”Ÿå®ç°å¯¹æ¯”
- [ ] è¾¹ç•Œæ¡ä»¶æµ‹è¯•

#### ä»»åŠ¡ 2.3.2ï¼šæ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†
**ä¼˜å…ˆçº§ï¼šä¸­ ğŸŸ¡**

`tests/backward/test_performance.py`:
```python
import torch
import time
import pytest
from flash_attn.backward.autograd_functions import flash_attn_func_with_grad

class TestBackwardPerformance:
    """æµ‹è¯•åå‘ä¼ æ’­æ€§èƒ½"""
    
    @pytest.mark.parametrize("seq_len", [128, 512, 1024, 2048])
    def test_backward_timing(self, seq_len):
        """æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„åå‘ä¼ æ’­æ—¶é—´"""
        device = torch.device('cuda')
        B, H, D = 4, 8, 64
        
        q = torch.randn(B, seq_len, H, D, device=device, requires_grad=True)
        k = torch.randn(B, seq_len, H, D, device=device, requires_grad=True)
        v = torch.randn(B, seq_len, H, D, device=device, requires_grad=True)
        
        # é¢„çƒ­
        for _ in range(10):
            out = flash_attn_func_with_grad(q, k, v)
            loss = out.sum()
            loss.backward()
            q.grad = None
            k.grad = None
            v.grad = None
        
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(100):
            out = flash_attn_func_with_grad(q, k, v)
            loss = out.sum()
            loss.backward()
            q.grad = None
            k.grad = None
            v.grad = None
            
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 100 * 1000  # ms
        print(f"åºåˆ—é•¿åº¦ {seq_len}: å¹³å‡åå‘ä¼ æ’­æ—¶é—´ {avg_time:.2f} ms")
        
        # æ€§èƒ½è¦æ±‚ï¼ˆè¿™äº›æ•°å­—éœ€è¦æ ¹æ®å®é™…æµ‹è¯•è°ƒæ•´ï¼‰
        if seq_len <= 512:
            assert avg_time < 50  # å°äº 50ms
        elif seq_len <= 1024:
            assert avg_time < 200  # å°äº 200ms
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        device = torch.device('cuda')
        B, L, H, D = 4, 1024, 8, 64
        
        # é‡ç½®å†…å­˜ç»Ÿè®¡
        torch.cuda.reset_peak_memory_stats()
        
        q = torch.randn(B, L, H, D, device=device, requires_grad=True)
        k = torch.randn(B, L, H, D, device=device, requires_grad=True)
        v = torch.randn(B, L, H, D, device=device, requires_grad=True)
        
        out = flash_attn_func_with_grad(q, k, v)
        loss = out.sum()
        loss.backward()
        
        peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
        print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {peak_memory:.2f} MB")
        
        # å†…å­˜ä½¿ç”¨ä¸åº”è¯¥è¿‡é«˜
        expected_memory = B * L * H * D * 4 * 6 / 1024 / 1024  # å¤§è‡´ä¼°ç®—
        assert peak_memory < expected_memory * 3  # å…è®¸ 3 å€çš„å¼€é”€
```

**è¾“å‡ºè¦æ±‚ï¼š**
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] å†…å­˜ä½¿ç”¨åˆ†æ
- [ ] ä¸æœŸæœ›æ€§èƒ½çš„å¯¹æ¯”
- [ ] æ€§èƒ½å›å½’æ£€æµ‹

## é˜¶æ®µ 3ï¼šå˜é•¿åºåˆ—åå‘ä¼ æ’­ï¼ˆé¢„ä¼° 1-2 å‘¨ï¼‰

### ä»»åŠ¡ 3.1ï¼šå®ç°å˜é•¿åºåˆ—åå‘ä¼ æ’­
**ä¼˜å…ˆçº§ï¼šä¸­ ğŸŸ¡**

```python
# flash_attn/backward/flaggems_backward.py ä¸­æ·»åŠ 

def flaggems_flash_attn_varlen_backward(
    dout: torch.Tensor,
    q: torch.Tensor,
    k: torch.Tensor,
    v: torch.Tensor,
    out: torch.Tensor,
    softmax_lse: torch.Tensor,
    cu_seqlens_q: torch.Tensor,
    cu_seqlens_k: torch.Tensor,
    max_seqlen_q: int,
    max_seqlen_k: int,
    dq: Optional[torch.Tensor] = None,
    dk: Optional[torch.Tensor] = None,
    dv: Optional[torch.Tensor] = None,
    causal: bool = False,
    softmax_scale: Optional[float] = None,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """å˜é•¿åºåˆ—çš„åå‘ä¼ æ’­å®ç°"""
    
    # TODO: å®ç°å˜é•¿åºåˆ—åå‘ä¼ æ’­
    # å…³é”®æŒ‘æˆ˜ï¼š
    # 1. å¤„ç†ä¸åŒé•¿åº¦åºåˆ—çš„æ¢¯åº¦è®¡ç®—
    # 2. å†…å­˜é«˜æ•ˆçš„å®ç°
    # 3. ä¸å‰å‘ä¼ æ’­çš„ä¸€è‡´æ€§
    
    # æš‚æ—¶ä½¿ç”¨æ‰¹æ¬¡è½¬æ¢çš„å›é€€æ–¹æ¡ˆ
    batch_size = len(cu_seqlens_q) - 1
    
    # è½¬æ¢ä¸ºæ‰¹æ¬¡æ ¼å¼ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œå†è½¬æ¢å›å˜é•¿æ ¼å¼
    # å…·ä½“å®ç°ç»†èŠ‚å¾…å®Œæˆ
    
    raise NotImplementedError("å˜é•¿åºåˆ—åå‘ä¼ æ’­å¾…å®ç°")
```

## é˜¶æ®µ 4ï¼šKV ç¼“å­˜å®ç°ï¼ˆé¢„ä¼° 2-3 å‘¨ï¼‰

### ä»»åŠ¡ 4.1ï¼šKV ç¼“å­˜æ¶æ„è®¾è®¡
**ä¼˜å…ˆçº§ï¼šä½ ğŸŸ¢**

```bash
# åˆ›å»º KV ç¼“å­˜æ¨¡å—
mkdir -p flash_attn/kvcache
touch flash_attn/kvcache/__init__.py
touch flash_attn/kvcache/cache_manager.py
touch flash_attn/kvcache/kv_attention.py
```

**å¾…å®ç°çš„åŠŸèƒ½ï¼š**
- [ ] KV ç¼“å­˜ç®¡ç†å™¨
- [ ] ç¼“å­˜æ›´æ–°ç­–ç•¥
- [ ] å†…å­˜ä¼˜åŒ–
- [ ] ä¸ç°æœ‰ API çš„é›†æˆ

## å¿«é€Ÿå¼€å§‹æŒ‡å—

### ç«‹å³å¯ä»¥å¼€å§‹çš„ä»»åŠ¡

1. **ä»»åŠ¡ 2.1.1**: FlagGems æºç åˆ†æ
   ```bash
   cd /home/Master/YangKY/.code/library/FlagGems
   find . -name "*.py" -exec grep -l "backward" {} \; > backward_files.txt
   cat backward_files.txt
   ```

2. **ä»»åŠ¡ 2.1.2**: å­¦ä¹  Flash Attention åå‘ä¼ æ’­ç®—æ³•
   - é˜…è¯»è®ºæ–‡ç¬¬ 3.2 èŠ‚
   - åˆ†æåŸå§‹ CUDA ä»£ç ä¸­çš„åå‘ä¼ æ’­å®ç°
   - ç”¨ PyTorch å®ç°ä¸€ä¸ªç®€å•çš„å‚è€ƒç‰ˆæœ¬

3. **ä»»åŠ¡ 2.2.1**: åˆ›å»ºæ¨¡å—ç»“æ„
   ```bash
   cd /home/Master/YangKY/.code/library/flash-attention-plus
   mkdir -p flash_attn/backward tests/backward
   # åˆ›å»ºåŸºç¡€æ–‡ä»¶
   ```

### éªŒè¯å½“å‰å®ç°

è¿è¡Œä»¥ä¸‹å‘½ä»¤ç¡®ä¿å½“å‰çš„å‰å‘ä¼ æ’­å®ç°æ­£å¸¸å·¥ä½œï¼š

```bash
micromamba activate dev
cd /home/Master/YangKY/.code/library/flash-attention-plus

python -c "
import torch
import flash_attn

# æµ‹è¯•æ‰€æœ‰å‰å‘ä¼ æ’­åŠŸèƒ½
device = torch.device('cuda')

# 1. æ ‡å‡†æ³¨æ„åŠ›
q = torch.randn(2, 128, 8, 64, dtype=torch.float16, device=device)
k = torch.randn(2, 128, 8, 64, dtype=torch.float16, device=device)
v = torch.randn(2, 128, 8, 64, dtype=torch.float16, device=device)
out = flash_attn.flash_attn_func(q, k, v, causal=True)
print(f'âœ… æ ‡å‡†æ³¨æ„åŠ›: {out.shape}')

# 2. å˜é•¿åºåˆ—
q_varlen = torch.randn(256, 8, 64, dtype=torch.float16, device=device)
k_varlen = torch.randn(256, 8, 64, dtype=torch.float16, device=device)
v_varlen = torch.randn(256, 8, 64, dtype=torch.float16, device=device)
cu_seqlens = torch.tensor([0, 64, 128, 192, 256], dtype=torch.int32, device=device)
out_varlen = flash_attn.flash_attn_varlen_func(
    q_varlen, k_varlen, v_varlen, cu_seqlens, cu_seqlens, 64, 64, causal=True
)
print(f'âœ… å˜é•¿åºåˆ—: {out_varlen.shape}')

print('ğŸ‰ å½“å‰å®ç°å·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥å¼€å§‹åå‘ä¼ æ’­å¼€å‘ï¼')
"
```

### æ¨èçš„å®ç°é¡ºåº

1. **Week 1**: ä»»åŠ¡ 2.1.1 å’Œ 2.1.2 - è°ƒç ”å’Œå­¦ä¹ 
2. **Week 2**: ä»»åŠ¡ 2.1.3 å’Œ 2.2.1 - è®¾è®¡å’Œç»“æ„æ­å»º
3. **Week 3**: ä»»åŠ¡ 2.2.2 - PyTorch å‚è€ƒå®ç°
4. **Week 4**: ä»»åŠ¡ 2.2.3 - FlagGems é›†æˆå°è¯•
5. **Week 5**: ä»»åŠ¡ 2.2.4 å’Œ 2.3.1 - AutoGrad é›†æˆå’Œæµ‹è¯•
6. **Week 6**: ä»»åŠ¡ 2.3.2 å’Œä¼˜åŒ– - æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜

æ¯å®Œæˆä¸€ä¸ªä»»åŠ¡ï¼Œè®°å¾—æ›´æ–°è¿™ä¸ªæ¸…å•å¹¶æäº¤ä»£ç ï¼

## è¿›åº¦è·Ÿè¸ª

è¯·åœ¨å®Œæˆæ¯ä¸ªä»»åŠ¡åï¼Œåœ¨å¯¹åº”çš„ `[ ]` ä¸­æ‰“ä¸Š `[x]` æ ‡è®°è¿›åº¦ã€‚

å½“å‰æ•´ä½“è¿›åº¦ï¼š**ç¬¬ä¸€é˜¶æ®µå®Œæˆ âœ…ï¼Œç¬¬äºŒé˜¶æ®µç­‰å¾…å¼€å§‹ ğŸš§**