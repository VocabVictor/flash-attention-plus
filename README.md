# FlashAttention-Plus

**åŸºäº FlagGems/Triton åç«¯çš„ç¡¬ä»¶æ— å…³ FlashAttention å®ç°**

[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)
[![Documentation](https://img.shields.io/badge/docs-GitHub%20Pages-blue)](https://vocabvictor.github.io/flash-attention-plus/)

[ğŸ“– English Documentation](README_EN.md) | [ä¸­æ–‡æ–‡æ¡£](README_CN.md)

## é¡¹ç›®æ¦‚è¿°

FlashAttention-Plus æ˜¯åŸå§‹ [FlashAttention](https://github.com/Dao-AILab/flash-attention) çš„ç›´æ¥æ›¿ä»£å“ï¼Œä½¿ç”¨ [FlagGems](https://github.com/FlagOpen/FlagGems) çš„ Triton å®ç°æ›¿ä»£ NVIDIA CUDA å†…æ ¸ã€‚è¿™ä½¿å¾— FlashAttention èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„ç¡¬ä»¶ä¸Šè¿è¡Œï¼ŒåŒæ—¶ä¿æŒ API å…¼å®¹æ€§ã€‚

**ä¸»è¦ç‰¹æ€§ï¼š**
- ğŸš€ **ç¡¬ä»¶æ— å…³**ï¼šä½¿ç”¨ Triton è€Œé CUDA ä¸“ç”¨ä»£ç 
- ğŸ”„ **API å…¼å®¹**ï¼šåŸå§‹ FlashAttention çš„ç›´æ¥æ›¿ä»£å“
- âš¡ **é«˜æ€§èƒ½**ï¼šåˆ©ç”¨ FlagGems ä¼˜åŒ–çš„ Triton å†…æ ¸
- ğŸ¯ **æ˜“äºé›†æˆ**ï¼šåªéœ€æœ€å°‘çš„ä»£ç æ›´æ”¹

## å®‰è£…è¯´æ˜

### ç¯å¢ƒè¦æ±‚

```bash
# æ”¯æŒ CUDA çš„ PyTorch
pip install torch>=2.0.0

# Triton (FlagGems æ‰€éœ€)
pip install triton>=3.0.0

# å…¶ä»–ä¾èµ–
pip install einops
```

### å®‰è£… FlagGems

```bash
cd ~/.code/library/FlagGems  # æˆ–ä½ å–œæ¬¢çš„ä½ç½®
git clone https://github.com/FlagOpen/FlagGems.git
cd FlagGems
pip install -e .
```

### å®‰è£… FlashAttention-Plus

```bash
git clone https://github.com/VocabVictor/flash-attention-plus.git
cd flash-attention-plus
pip install -e .
```

## ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

å½“å‰ç‰ˆæœ¬ç›´æ¥ä½¿ç”¨ FlagGems åç«¯ï¼Œæ— éœ€è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```python
import torch
from flash_attn import flash_attn_func

# åˆ›å»ºå¼ é‡ (å¿…é¡»æ˜¯ fp16 æˆ– bf16)
q = torch.randn(2, 1024, 16, 64, device='cuda', dtype=torch.float16)
k = torch.randn(2, 1024, 16, 64, device='cuda', dtype=torch.float16)
v = torch.randn(2, 1024, 16, 64, device='cuda', dtype=torch.float16)

# è¿è¡Œ flash attention
output = flash_attn_func(q, k, v, causal=True)
```

### æ”¯æŒçš„åŠŸèƒ½

```python
# æ ‡å‡†æ³¨æ„åŠ›
output = flash_attn_func(q, k, v, causal=True)

# QKV æ‰“åŒ…æ ¼å¼
qkv = torch.randn(2, 1024, 3, 16, 64, device='cuda', dtype=torch.float16)
output = flash_attn_qkvpacked_func(qkv, causal=True)

# å˜é•¿åºåˆ—
output = flash_attn_varlen_func(q_varlen, k_varlen, v_varlen, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k)
```

## ä¸åŸå§‹ FlashAttention çš„ä¸»è¦å·®å¼‚

### å·²æ›´æ”¹çš„éƒ¨åˆ†
- **åç«¯**ï¼šCUDA å†…æ ¸ â†’ FlagGems/Triton å†…æ ¸
- **ç¡¬ä»¶æ”¯æŒ**ï¼šä»…æ”¯æŒ NVIDIA â†’ ç¡¬ä»¶æ— å…³
- **å®‰è£…**ï¼šæ— éœ€ CUDA ç¼–è¯‘

### ä¿ç•™çš„éƒ¨åˆ†
- âœ… API å…¼å®¹æ€§
- âœ… æ ¸å¿ƒåŠŸèƒ½
- âœ… æ¨¡å‹æ”¯æŒ (BERT, GPT, LLaMA ç­‰)
- âœ… æ€§èƒ½ç‰¹æ€§

### å½“å‰çŠ¶æ€
- âœ… **å‰å‘ä¼ æ’­**ï¼šå®Œå…¨å®ç°
  - æ ‡å‡†æ³¨æ„åŠ› (flash_attn_func)
  - QKV æ‰“åŒ…æ ¼å¼ (flash_attn_qkvpacked_func)
  - KV æ‰“åŒ…æ ¼å¼ (flash_attn_kvpacked_func)
  - å˜é•¿åºåˆ— (flash_attn_varlen_func)
- âŒ **åå‘ä¼ æ’­**ï¼šæœªå®ç° (ä»…æ¨ç†)
- âŒ **KV ç¼“å­˜**ï¼šå¾…å¼€å‘

## ç¤ºä¾‹

æŸ¥çœ‹æ›´å¤šä½¿ç”¨ç¤ºä¾‹è¯·å‚è€ƒæ–‡æ¡£ï¼š
- `README_CN.md` - è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—
- `IMPLEMENTATION_PLAN_CN.md` - å®ç°è®¡åˆ’
- `TASK_CHECKLIST_CN.md` - å¼€å‘ä»»åŠ¡æ¸…å•

## æ€§èƒ½

æ€§èƒ½å› ç¡¬ä»¶å’Œé…ç½®è€Œå¼‚ã€‚é¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼ˆç”±äº Triton å†…æ ¸ç¼–è¯‘ï¼‰ï¼Œä½†åç»­è¿è¡Œä½¿ç”¨ç¼“å­˜å†…æ ¸ã€‚

## æŠ€æœ¯ç»†èŠ‚

æ›´å¤šå…³äº FlagGems é›†æˆçš„ä¿¡æ¯ï¼Œè¯·å‚é˜…ï¼š
- `PROJECT_STATUS_CN.md` - é¡¹ç›®çŠ¶æ€æŠ¥å‘Š
- `IMPLEMENTATION_PLAN_CN.md` - è¯¦ç»†æŠ€æœ¯è®¡åˆ’

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ä¸åŸå§‹ FlashAttention ç›¸åŒçš„ BSD 3-Clause è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE)ã€‚

## è‡´è°¢

- åŸå§‹ FlashAttention ç”± [Tri Dao](https://tridao.me/) å’Œå›¢é˜Ÿå¼€å‘
- [FlagGems](https://github.com/FlagOpen/FlagGems) å›¢é˜Ÿæä¾› Triton å†…æ ¸
- [OpenAI Triton](https://github.com/openai/triton) GPU ç¼–ç¨‹è¯­è¨€

## å‘å±•è·¯çº¿å›¾

- [ ] å®ç°åå‘ä¼ æ’­æ”¯æŒ
- [ ] æ·»åŠ  KV ç¼“å­˜åŠŸèƒ½
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] å…¨é¢çš„åŸºå‡†æµ‹è¯•
- [ ] æ”¯æŒæ›´å¤šç¡¬ä»¶åç«¯

è¯¦ç»†çš„å¼€å‘è®¡åˆ’è¯·å‚é˜… `IMPLEMENTATION_PLAN_CN.md`ã€‚