# FlashAttention-Plus

**åŸºäº FlagGems/Triton åç«¯çš„ç¡¬ä»¶æ— å…³ FlashAttention å®ç°**

[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)

## æ¦‚è¿°

FlashAttention-Plus æ˜¯åŸå§‹ [FlashAttention](https://github.com/Dao-AILab/flash-attention) çš„ç›´æ¥æ›¿ä»£å“ï¼Œå®ƒä½¿ç”¨ [FlagGems](https://github.com/FlagOpen/FlagGems) çš„ Triton å®ç°æ›¿æ¢äº† NVIDIA CUDA å†…æ ¸ã€‚è¿™ä½¿å¾— FlashAttention èƒ½å¤Ÿåœ¨æ›´å¹¿æ³›çš„ç¡¬ä»¶ä¸Šè¿è¡Œï¼ŒåŒæ—¶ä¿æŒ API å…¼å®¹æ€§ã€‚

**ä¸»è¦ç‰¹æ€§ï¼š**

- ğŸš€ **ç¡¬ä»¶æ— å…³**ï¼šä½¿ç”¨ Triton è€Œé CUDA ç‰¹å®šä»£ç 
- ğŸ”„ **API å…¼å®¹**ï¼šå¯ç›´æ¥æ›¿æ¢åŸå§‹ FlashAttention
- âš¡ **é«˜æ€§èƒ½**ï¼šåˆ©ç”¨ FlagGems çš„ä¼˜åŒ– Triton å†…æ ¸
- ğŸ¯ **æ˜“äºé›†æˆ**ï¼šåªéœ€æœ€å°‘çš„ä»£ç æ›´æ”¹

## ä¸ºä»€ä¹ˆé€‰æ‹© FlashAttention-Plusï¼Ÿ

åŸå§‹çš„ FlashAttention å®ç°æä¾›äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶ CUDA ç‰¹å®šçš„å†…æ ¸ï¼Œä»…é™äº NVIDIA GPUã€‚FlashAttention-Plus é€šè¿‡ä½¿ç”¨ FlagGems åŸºäº Triton çš„å®ç°æ¥è§£å†³è¿™ä¸€é™åˆ¶ï¼Œè¿™å¯èƒ½åœ¨å„ç§ç¡¬ä»¶åŠ é€Ÿå™¨ä¸Šè¿è¡Œï¼ŒåŒæ—¶ä¿æŒç›¸åŒçš„ APIã€‚

## å¿«é€Ÿç¤ºä¾‹

```python
import os
import torch

# å¯ç”¨ FlagGems åç«¯
os.environ["FLASH_ATTENTION_USE_FLAGGEMS"] = "TRUE"

from flash_attn import flash_attn_func

# åˆ›å»ºå¼ é‡ï¼ˆå¿…é¡»æ˜¯ fp16 æˆ– bf16ï¼‰
batch_size, seq_len, num_heads, head_dim = 2, 1024, 16, 64
q = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
k = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)
v = torch.randn(batch_size, seq_len, num_heads, head_dim, 
                device='cuda', dtype=torch.float16)

# è¿è¡Œ flash attention
output = flash_attn_func(q, k, v, causal=True)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
```

## å¿«é€Ÿå¼€å§‹

- [å®‰è£…æŒ‡å—](installation.md) - è®¾ç½® FlashAttention-Plus
- [ä½¿ç”¨æŒ‡å—](usage.md) - å­¦ä¹ å¦‚ä½•ä½¿ç”¨ FlashAttention-Plus
- [è¿ç§»æŒ‡å—](migration.md) - ä»åŸå§‹ FlashAttention è¿ç§»
- [API å‚è€ƒ](api.md) - è¯¦ç»†çš„ API æ–‡æ¡£

## é¡¹ç›®çŠ¶æ€

æœ¬é¡¹ç›®æ­£åœ¨ç§¯æå¼€å‘ä¸­ã€‚å½“å‰é™åˆ¶åŒ…æ‹¬ï¼š

- âŒ å°šæœªå®ç°åå‘ä¼ æ’­
- âŒ KV ç¼“å­˜æ”¯æŒå¾…å®š
- âŒ ä¸æ”¯æŒå¯å˜é•¿åº¦åºåˆ—
- âš ï¸ Dropout æ¥å£å­˜åœ¨ä½†å¯èƒ½åŠŸèƒ½ä¸å®Œæ•´

æŸ¥çœ‹æˆ‘ä»¬çš„[è·¯çº¿å›¾](#è·¯çº¿å›¾)äº†è§£å³å°†æ¨å‡ºçš„åŠŸèƒ½ã€‚

## è·¯çº¿å›¾

- [ ] å®ç°åå‘ä¼ æ’­æ”¯æŒ
- [ ] æ·»åŠ  KV ç¼“å­˜åŠŸèƒ½
- [ ] æ”¯æŒå¯å˜é•¿åº¦åºåˆ—
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] å…¨é¢çš„åŸºå‡†æµ‹è¯•
- [ ] æ”¯æŒæ›´å¤šç¡¬ä»¶åç«¯

## è®¸å¯è¯

æœ¬é¡¹ç›®ä¸åŸå§‹ FlashAttention ä¿æŒç›¸åŒçš„ BSD 3-Clause è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](https://github.com/VocabVictor/flash-attention-plus/blob/main/LICENSE)ã€‚

## è‡´è°¢

- åŸå§‹ FlashAttention ç”± [Tri Dao](https://tridao.me/) åŠå…¶å›¢é˜Ÿå¼€å‘
- [FlagGems](https://github.com/FlagOpen/FlagGems) å›¢é˜Ÿæä¾› Triton å†…æ ¸
- [OpenAI Triton](https://github.com/openai/triton) æä¾› GPU ç¼–ç¨‹è¯­è¨€